{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5166427,"sourceType":"datasetVersion","datasetId":3002552}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install  ctgan==0.5.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install --force-reinstall --no-deps six==1.16.0 --yes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ydata-synthetic==0.7.0","metadata":{"id":"3N4I5Ju7fUcu","outputId":"a07447f6-cb91-4fab-a57e-5145fb41242c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install matplotlib==3.1.3","metadata":{"id":"XODEPptW7_Eg","outputId":"4c2c8231-468d-4d01-bc95-26a7e44ae1e8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install --force-reinstall --no-deps numpy==1.21.6  --yes\n!conda install --force-reinstall  pandas==1.1.5 --yes\n!pip install --force-reinstall  pandas==1.3.5 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -V","metadata":{"id":"cYnVXmmopWEV","outputId":"cce53638-6a2e-4dd1-bdab-34f23edbbbac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install xgboost==1.5.0","metadata":{"id":"FqgVqHq_yu73","outputId":"7a23b40f-8cbd-411a-afaf-227662cb20e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the CTGAN Synthesizer model from ctgan library\nfrom ctgan import CTGANSynthesizer\n\n# Importing the GAN,CGAN,DRAGAN,CRAMERGAN,WGAN Synthesizer model from ydata_synthetic library\nfrom ydata_synthetic.synthesizers import ModelParameters, TrainParameters\nfrom ydata_synthetic.synthesizers.regular import WGAN\nfrom ydata_synthetic.synthesizers.regular import  VanilllaGAN,CGAN,DRAGAN,CRAMERGAN\n","metadata":{"id":"1p8lzdZ2efJB","outputId":"0a62f6c1-4f68-4b5c-f15a-76ef658a367f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the libraries\nimport pandas as pd #Using for loading and saving dataset\nfrom sklearn.utils import shuffle\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport os\n# Cross Validation / Splitting the dataset into the Training set and Test set / Normalization\nfrom sklearn.model_selection import cross_val_score, KFold,StratifiedKFold\n\nfrom sklearn.model_selection import train_test_split  \nfrom sklearn.preprocessing import StandardScaler\n\n# plots and metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix,classification_report,auc\nfrom sklearn.metrics import recall_score,precision_score,precision_recall_fscore_support,roc_auc_score,roc_curve\ntf.random.set_seed(0)\nnp.random.seed(0)\n# Importing the Classifer Models\nimport xgboost as xgb\nfrom IPython.display import display, HTML\n\nfrom sklearn import svm\nplt.rcParams[\"figure.figsize\"] = (10,8)\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('Libraries have been imported')","metadata":{"id":"UIh9uUpkefJB","outputId":"22f2845e-f11f-41fa-d9cd-01d75370ae18","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -V","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the dataset and Pre-processing\ndef prepare_dataset(data_name,n_splits):\n  num_cols=[]\n  cat_cols=[]\n  if data_name=='Breast':\n    df = pd.read_csv(\"/kaggle/input/dataset4/Breast Cancer Dataset/data.csv\") ###The path for dataset\n    #Drop irrelevant features\n    list_drop = ['id','Unnamed: 32','area_mean','radius_mean','perimeter_worst','radius_worst','area_worst','radius_se',\n                 'perimeter_se','concave points_mean','compactness_worst','concave points_se','concavity_se','concavity_worst',\n                 'fractal_dimension_worst','smoothness_mean','symmetry_mean']\n    df.drop(list_drop,axis=1,inplace=True)\n        #Encode categorical feature\n    df['diagnosis'].replace([\"B\", \"M\"], [0,1], inplace = True)\n    # df =df.astype({'diagnosis': np.int8}) ### fixing the datatypes for dataframe \n    num_cols = list(df.columns[ df.columns != 'diagnosis' ])\n    cat_cols = ['diagnosis']\n    target_label='diagnosis'\n    folder_name = 'Breast Cancer Dataset'\n    !mkdir 'Breast Cancer Dataset'\n  elif data_name=='Lung':\n    df = pd.read_excel(\"/kaggle/input/dataset4/Lung Cancer Dataset/cancer patient data sets.xlsx\") ###The path for dataset\n        #Drop irrelevant features\n    list_drop = ['Patient Id']\n    df.drop(list_drop,axis=1,inplace=True)\n    #Encode categorical feature\n    df['Level'].replace([\"Low\", \"Medium\",\"High\"], [0,1,2], inplace = True)\n    # Get Unique Count using Series.unique()\n    for i  in df.columns.tolist():\n      if df[i].unique().size > 6 :\n        num_cols.append(i)\n        print(f'Unique values count :{i} => {str(df[i].unique().size)} Added to num_cols')\n      else :\n        cat_cols.append(i)\n        print(f'Unique values count :{i} => {str(df[i].unique().size)} Added to cat_cols')\n    target_label='Level'\n    folder_name = 'Lung Cancer Dataset'\n    !mkdir 'Lung Cancer Dataset'\n\n  elif data_name=='CTG':\n    df = pd.read_csv(\"/kaggle/input/dataset4/CTG Dataset/CTG.csv\") ###The path for dataset\n    #Drop irrelevant features\n    list_drop = [\"FileName\",\"Date\",\"SegFile\",\"b\",\"e\",\"A\", \"B\",\"C\", \"D\" ,\"E\", \"AD\", \"DE\" ,\"LD\", \"FS\", \"SUSP\",\"CLASS\",\"DR\"]\n    df.drop(list_drop,axis=1,inplace=True)\n    # process to delete all nan data\n    df = df.dropna()\n    df =df.astype({'NSP': int}) ### fixing the datatypes for dataframe \n\n    # Get Unique Count using Series.unique()\n    for i  in df.columns.tolist():\n      if df[i].unique().size > 6 :\n        num_cols.append(i)\n        print(f'Unique values count :{i} => {str(df[i].unique().size)} Added to num_cols')\n      else :\n        cat_cols.append(i)\n        print(f'Unique values count :{i} => {str(df[i].unique().size)} Added to cat_cols')\n    target_label='NSP'\n    folder_name = 'CTG Dataset'\n    !mkdir 'CTG Dataset'\n  return df,target_label,num_cols,cat_cols,folder_name,n_splits","metadata":{"id":"XIM71TB3084V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read data data.csv\ndf,target_label,num_cols,cat_cols,folder_name ,n_splits= prepare_dataset('Breast',5)\ndf.sample(5)","metadata":{"id":"94uBlnWxw7pt","outputId":"8652f79f-4ce7-4efb-f7ff-5071cdd0595d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"0aoL6obwefJH","outputId":"1589aa6b-05f4-40c5-f22c-b29d24c0f8eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check transparency of all columns\ndf.describe(include='all').T","metadata":{"id":"gsP11xQExjrz","outputId":"5c021eed-9532-4c3b-fdd2-ee4fd3942678","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[target_label].value_counts()","metadata":{"id":"g62tccqAtpvc","outputId":"a55b1e38-167c-4b72-ebd5-674da82fe1ab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_up(data):\n    #Select all features  except target\n    X_data = data.drop([target_label],axis=1)\n    #Select target variable\n    y_data = data[target_label]\n      #### Splitting the dataset into training and testing 80%,20%\n    X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, \n                                                          test_size = 0.2, \n                                                          random_state = 0\n\n                                                        )\n      # Feature Scaling\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    return X_train, X_test, y_train, y_test ","metadata":{"id":"7UmfjvvPRMFr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sensitivity1(y_true,y_pred):\n    cm=confusion_matrix(y_true, y_pred)\n    FP = cm.sum(axis=0) - np.diag(cm)  \n    FN = cm.sum(axis=1) - np.diag(cm)\n    TP = np.diag(cm)\n    TN = cm.sum() - (FP + FN + TP)\n    Sensitivity = TP/(TP+FN)    \n    return np.mean(Sensitivity)\n\ndef specificity1(y_true,y_pred):\n    cm=confusion_matrix(y_true, y_pred)\n    FP = cm.sum(axis=0) - np.diag(cm)  \n    FN = cm.sum(axis=1) - np.diag(cm)\n    TP = np.diag(cm)\n    TN = cm.sum() - (FP + FN + TP)\n    Specificity = TN/(TN+FP)    \n    return np.mean(Specificity)\n","metadata":{"id":"uG5x8cijtXWz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training & Evaluation Using Classifiers: xgboost, SVM\n# Splitting the dataset into training and testing 80% ,20%\nX_train, X_test, y_train, y_test = split_up(df)","metadata":{"id":"xxJMcI9JefJK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBoost Classifier\n#Train the data\nstart=time.time()\nxgb_model = xgb.XGBClassifier(random_state=0,eval_metric='mlogloss').fit(X_train, y_train)\n\nprint('\\n Training time {:.2f} millisecond'.format((time.time()-start)*1000))\nprint(\"\\n Training Accuracy: {:.2f} %\".format(xgb_model.score(X_train, y_train)*100))\n    \n# Applying k-Fold Cross Validation\nskf = StratifiedKFold(n_splits=n_splits,random_state=None, shuffle=True) \naccuracies = []\n\nfor train_index, test_index in skf.split(X_train,y_train): \n\n    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n    y_train_fold, y_test_fold =  y_train.iloc[train_index], y_train.iloc[test_index]\n    xgb_model.fit(X_train_fold, y_train_fold) \n    accuracies.append(xgb_model.score(X_test_fold, y_test_fold))\n\n\n# accuracies = cross_val_score(xgb_model, X = X_train, y = y_train, cv=KFold(n_splits=10, random_state=None, shuffle=True))\nprint(\"\\n Accuracy for Cross Validation: {:.2f} %\".format(np.array(accuracies).mean()*100))","metadata":{"id":"Yu1s7T4qefJL","outputId":"c4f1dc78-c102-42f0-e394-75e0af6917a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Evaluation\n#making prediction\nstart=time.time()\npredicted= xgb_model.predict(X_test)\nprint('\\n Testing time {:.2f} millisecond'.format((time.time()-start)*1000))\n\ntest_time =(time.time()-start)*1000\n\n #calculating accuracy\nprint(' Test accuracy is {:.2f}%'.format(accuracy_score(y_test,predicted)*100))\nreport = classification_report(y_test, predicted)\nprint(report)\n\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, predicted)\nprint(\"\\n Confusion Matrix:\")\nsns.heatmap(cm,annot=True,fmt='g', cmap='Blues')","metadata":{"id":"n38cwIpvefJL","outputId":"fa0e2388-1c47-4c21-a2d9-78cec2957a64","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Precision Score: {:.2f}%\".format(precision_score(y_test, predicted, average='weighted') * 100))  ##Precision = TP/(TP+FP)\nprint(\" Recall Score: {:.2f}%\".format(recall_score(y_test, predicted, average='weighted') * 100))   ##Recall= TP / (TP + FN)\nprint(\" F1 Score: {:.2f}%\".format(f1_score(y_test, predicted, average='weighted') * 100)) #### F1 = 2*[Recall*Precision]/(Recall+Precision)\n\n\nif target_label =='diagnosis':\n    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])  ##TP/(TP+FN)\n    specificity = cm[1,1]/(cm[1,0]+cm[1,1]) ##TN/(TN+FP)\nelse :\n    sensitivity= sensitivity1(y_test, predicted)\n    specificity=  specificity1(y_test, predicted)\n\nprint('\\n Sensitivity Score: {:.2f}%'.format(sensitivity*100))\nprint(' Specificity Score: {:.2f}%'.format(specificity*100))","metadata":{"id":"2mVJ0o0jDpZJ","outputId":"eb700912-1e7e-4684-df8f-7d8a60c0aa02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVM Classifier\n#Train the data\nstart=time.time()\nsvm_model = svm.SVC(random_state=0).fit(X_train, y_train)\nprint('\\n Training time {:.2f} millisecond'.format((time.time()-start)*1000))\n\nprint(\"\\n Training Accuracy: {:.2f} %\".format(svm_model.score(X_train, y_train)*100))\n    \n# Applying k-Fold Cross Validation\nskf = StratifiedKFold(n_splits=n_splits,random_state=None, shuffle=True) \naccuracies = []\n\nfor train_index, test_index in skf.split(X_train,y_train): \n\n    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n    y_train_fold, y_test_fold =  y_train.iloc[train_index], y_train.iloc[test_index]\n    svm_model.fit(X_train_fold, y_train_fold) \n    accuracies.append(svm_model.score(X_test_fold, y_test_fold))\n\n\n# accuracies = cross_val_score(svm_model, X = X_train, y = y_train, cv=KFold(n_splits=5, random_state=None, shuffle=True))\nprint(\"\\n Accuracy for Cross Validation: {:.2f} %\".format(np.array(accuracies).mean()*100))","metadata":{"id":"DudMVwinefJM","outputId":"88481b93-459b-4e7a-e7ea-8b5949fdac89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Evaluation\n#making prediction\nstart=time.time()\npredicted= svm_model.predict(X_test)\nprint('\\n Testing time {:.2f} millisecond'.format((time.time()-start)*1000)) \n\n\n #calculating accuracy\nprint(' Test accuracy is {:.2f}%'.format(accuracy_score(y_test,predicted)*100))\nreport = classification_report(y_test, predicted)\nprint(report)\n\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, predicted)\nprint(\"\\n Confusion Matrix:\")\nsns.heatmap(cm,annot=True,fmt='g', cmap='Blues')","metadata":{"id":"eMgardNrefJM","outputId":"0d0ca152-4c43-4610-d21c-a24f64aa1bf3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Precision Score: {:.2f}%\".format(precision_score(y_test, predicted, average='weighted') * 100))  ##Precision = TP/(TP+FP)\nprint(\" Recall Score: {:.2f}%\".format(recall_score(y_test, predicted, average='weighted') * 100))   ##Recall= TP / (TP + FN)\nprint(\" F1 Score: {:.2f}%\".format(f1_score(y_test, predicted, average='weighted') * 100)) #### F1 = 2*[Recall*Precision]/(Recall+Precision)\nif target_label =='diagnosis':\n    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])  ##TP/(TP+FN)\n    specificity = cm[1,1]/(cm[1,0]+cm[1,1]) ##TN/(TN+FP)\nelse :\n    sensitivity= sensitivity1(y_test, predicted)\n    specificity=  specificity1(y_test, predicted)\n\nprint('\\n Sensitivity Score: {:.2f}%'.format(sensitivity*100))\nprint(' Specificity Score: {:.2f}%'.format(specificity*100))","metadata":{"id":"gkCHP6NPDpZK","outputId":"37932761-8406-4e68-a097-eadfd779f7b4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_results_dictionary(model):\n    return {\n        \n   \n    'GAN Methods':['Classification Model','Dataset','Cross-validation Value','Accuracy Rate','F1Score','Recall /Sensibility','Specificity','Average Time'],\n        \n    'Original':[model,'Original',f'CV={n_splits}'],\n    'GAN':[model,'Synthetic',f'CV={n_splits}'],\n    'CGAN':[model,'Synthetic',f'CV={n_splits}'],\n    'CTGAN':[model,'Synthetic',f'CV={n_splits}'],\n    'CRAMERGAN':[model,'Synthetic',f'CV={n_splits}'],\n    'DRAGAN':[model,'Synthetic',f'CV={n_splits}'],\n    'WGAN':[model,'Synthetic',f'CV={n_splits}'],\n    'GAN_Mixed':[model,'Mixed',f'CV={n_splits}'],\n    'CGAN_Mixed':[model,'Mixed',f'CV={n_splits}'],\n    'CTGAN_Mixed':[model,'Mixed',f'CV={n_splits}'],\n    'CRAMERGAN_Mixed':[model,'Mixed',f'CV={n_splits}'],\n    'DRAGAN_Mixed':[model,'Mixed',f'CV={n_splits}'],\n   'WGAN_Mixed':[model,'Mixed',f'CV={n_splits}'],\n\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results_xgb = generate_results_dictionary(\"XGBoost\")","metadata":{"id":"ijNdpO-jcE29","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results_svm= generate_results_dictionary(\"SVM\")","metadata":{"id":"Pnh_g_UqcGS-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_evaulate_xgb (X_train, X_test, y_train, y_test,df_type=''):\n\n    ##Train the data\n    start=time.time()\n    xgb_model = xgb.XGBClassifier(random_state=0,eval_metric='mlogloss').fit(X_train, y_train)\n\n    print(\"\\n Training Accuracy: {:.2f} %\".format(xgb_model.score(X_train, y_train)*100))\n      \n  # Applying k-Fold Cross Validation\n    skf = StratifiedKFold(n_splits=n_splits,random_state=None, shuffle=True) \n    accuracies = []\n\n    for train_index, test_index in skf.split(X_train,y_train): \n\n        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n        y_train_fold, y_test_fold =  y_train.iloc[train_index], y_train.iloc[test_index]\n        xgb_model.fit(X_train_fold, y_train_fold) \n        accuracies.append(xgb_model.score(X_test_fold, y_test_fold))\n\n\n    print(\"\\n Accuracy for Cross Validation: {:.2f} %\".format(np.array(accuracies).mean()*100))\n  ### Evaluation\n  #making prediction\n#   start=time.time()\n    predicted= xgb_model.predict(X_test)\n\n    average_time =((time.time()-start)*1000)/2\n\n  #calculating accuracy\n    print(' Test accuracy is {:.2f}%'.format(accuracy_score(y_test,predicted)*100))\n    report = classification_report(y_test, predicted)\n\n\n  # Making the Confusion Matrix\n    cm = confusion_matrix(y_test, predicted)\n    print(\"\\n Confusion Matrix:\")\n    sns.heatmap(cm,annot=True,fmt='g', cmap='Blues')\n    plt.show()\n\n    print(\"\\n Precision Score: {:.2f}%\".format(precision_score(y_test, predicted, average='weighted') * 100))  ##Precision = TP/(TP+FP)\n    print(\" Recall Score: {:.2f}%\".format(recall_score(y_test, predicted, average='weighted') * 100))   ##Recall= TP / (TP + FN)\n    print(\" F1 Score: {:.2f}%\".format(f1_score(y_test, predicted, average='weighted') * 100)) #### F1 = 2*[Recall*Precision]/(Recall+Precision)\n\n    if target_label =='diagnosis':\n        sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])  ##TP/(TP+FN)\n        specificity = cm[1,1]/(cm[1,0]+cm[1,1]) ##TN/(TN+FP)\n    else :\n        sensitivity= sensitivity1(y_test, predicted)\n        specificity=  specificity1(y_test, predicted)\n\n    print('\\n Sensitivity Score: {:.2f}%'.format(sensitivity*100))\n    print(' Specificity Score: {:.2f}%'.format(specificity*100))\n\n    accuracy =accuracy_score(y_test,predicted)*100\n    f1=f1_score(y_test, predicted, average='weighted') * 100\n    recall=recall_score(y_test, predicted, average='weighted') * 100\n    precision=precision_score(y_test, predicted, average='weighted') * 100\n    support =len(X_test)\n\n    \n    Results_xgb[df_type].append(round(accuracy,2))\n    Results_xgb[df_type].append(round(f1,2))\n    Results_xgb[df_type].append(round(recall,2))\n\n    Results_xgb[df_type].append(round(specificity*100,2))\n    Results_xgb[df_type].append(average_time)\n\n","metadata":{"id":"rPHhvwQ6cHUi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_evaulate_svm (X_train, X_test, y_train, y_test,df_type=''):\n\n      ##Train the data\n    start=time.time()\n    svm_model = svm.SVC(random_state=0).fit(X_train, y_train)\n\n    print(\"\\n Training Accuracy: {:.2f} %\".format(svm_model.score(X_train, y_train)*100))\n\n        # Applying k-Fold Cross Validation\n    skf = StratifiedKFold(n_splits=n_splits,random_state=None, shuffle=True) \n    accuracies = []\n\n    for train_index, test_index in skf.split(X_train,y_train): \n\n        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n        y_train_fold, y_test_fold =  y_train.iloc[train_index], y_train.iloc[test_index]\n        svm_model.fit(X_train_fold, y_train_fold) \n        accuracies.append(svm_model.score(X_test_fold, y_test_fold))\n\n\n    print(\"\\n Accuracy for Cross Validation: {:.2f} %\".format(np.array(accuracies).mean()*100))\n      ### Evaluation\n      #making prediction\n\n    predicted= svm_model.predict(X_test)\n\n    average_time =(time.time()-start)*1000\n      #calculating accuracy\n    print(' Test accuracy is {:.2f}%'.format(accuracy_score(y_test,predicted)*100))\n    report = classification_report(y_test, predicted)\n    print(report)\n\n\n      # Making the Confusion Matrix\n    cm = confusion_matrix(y_test, predicted)\n    print(\"\\n Confusion Matrix:\")\n    sns.heatmap(cm,annot=True,fmt='g', cmap='Blues')\n    plt.show()\n    print(\"\\n Precision Score: {:.2f}%\".format(precision_score(y_test, predicted, average='weighted') * 100))  ##Precision = TP/(TP+FP)\n    print(\" Recall Score: {:.2f}%\".format(recall_score(y_test, predicted, average='weighted') * 100))   ##Recall= TP / (TP + FN)\n    print(\" F1 Score: {:.2f}%\".format(f1_score(y_test, predicted, average='weighted') * 100)) #### F1 = 2*[Recall*Precision]/(Recall+Precision)\n\n    if target_label =='diagnosis':\n        sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])  ##TP/(TP+FN)\n        specificity = cm[1,1]/(cm[1,0]+cm[1,1]) ##TN/(TN+FP)\n    else :\n        sensitivity= sensitivity1(y_test, predicted)\n        specificity=  specificity1(y_test, predicted)\n    accuracy =accuracy_score(y_test,predicted)*100\n    f1=f1_score(y_test, predicted, average='weighted') * 100\n    recall=recall_score(y_test, predicted, average='weighted') * 100\n    precision=precision_score(y_test, predicted, average='weighted') * 100\n    support =len(X_test)\n    Results_svm[df_type].append(round(accuracy,2))\n    \n    Results_svm[df_type].append(round(f1,2))\n    Results_svm[df_type].append(round(recall,2))\n#     Results_svm[df_type].append(round(precision,2))\n#     Results_svm[df_type].append(support)\n    Results_svm[df_type].append(round(specificity*100,2))\n    Results_svm[df_type].append(average_time)\n#     Results_svm[df_type].append(round(np.array(accuracies).mean()*100,2))","metadata":{"id":"6nbiMcUNcIrU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets plot histogram of all plots\ndef plot_histogram(df,df_gan):\n  \n  all_col = df.select_dtypes(include=np.number).columns.tolist()\n  \n  for i in range(len(all_col)):\n      plt.subplot(2,2,1)\n      plt.hist(df[all_col[i]])\n      plt.tight_layout()\n      plt.title(all_col[i],fontsize=25)\n\n      plt.subplot(2,2,2)\n      plt.hist(df_gan[all_col[i]],color ='grey')\n\n      plt.tight_layout()\n      plt.title(f'Synthetic {all_col[i]}',fontsize=25)\n      plt.show()","metadata":{"id":"rAYDzZbDev14","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define training parameters\ntest_size = 212 \nnoise_dim = 256\ndim = 128\nbatch_size = 128\nlog_step = 500\nlearning_rate =  5e-6  \nbeta_1 = 0.5\nbeta_2 = 0.9\nmodels_dir = './cache'\ngan_args = ModelParameters(batch_size=batch_size,\n                          lr=learning_rate,\n                          betas=(beta_1, beta_2),\n                          noise_dim=noise_dim,\n                          layers_dim=dim)\n","metadata":{"id":"TvhdQkIPP4ru","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef drop_some_data(df,df_gan):\n  \n    for i in df[target_label].value_counts().items():\n        drop_examples_number = df_gan[target_label].value_counts()[i[0]]- i[1]\n        drop_indices = np.random.choice(df_gan.loc[df_gan[target_label] == i[0]].index, drop_examples_number, replace=False)\n        df_gan.drop(drop_indices,inplace= True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixed_df(df,df_gan):\n    mixed_wgan = pd.concat([df, df_gan])\n    return shuffle(mixed_wgan)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_wgan_data(df,num_cols,cat_cols):\n    wgan_model = WGAN\n  # WGAN training\n\n\n  #Training the WGAN model\n    wgan = wgan_model(gan_args, n_critic=10)\n\n    wgan.train(data = df, train_arguments=train_args, num_cols=num_cols, cat_cols=cat_cols)\n\n    df_wgan = wgan.sample(3*len(df)).abs()\n    drop_some_data(df,df_wgan)\n    mixed_wgan = mixed_df(df,df_wgan)\n    \n    return df_wgan, mixed_wgan\n","metadata":{"id":"a7JpwmatFrst","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_gan_data(df,num_cols,cat_cols):\n  ### Calling the GAN model\n    gan_model = VanilllaGAN\n\n    #Training the WGAN model\n    gan = gan_model(gan_args)\n    gan.train(data = df, train_arguments=train_args, num_cols=num_cols, cat_cols=cat_cols)\n    df_gan = gan.sample(4*len(df)).abs()\n    drop_some_data(df,df_gan)\n    \n\n    \n    return df_gan, mixed_df(df,df_gan)\n\n\n","metadata":{"id":"ltgVXQRo7lCG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_cgan_data(df,num_cols,cat_cols,label_col,epoch):\n    if target_label =='diagnosis':\n        train_args = TrainParameters(epochs=epoch,\n                              cache_prefix='',\n                              sample_interval=log_step,\n               \n                              labels=(0,1))\n        num_classes=2\n    elif target_label =='Level':\n        train_args = TrainParameters(epochs=epoch,\n                                cache_prefix='',\n                                sample_interval=log_step,\n                    \n                                labels=(0,1,2))\n        num_classes=3\n    elif target_label =='NSP':\n        train_args = TrainParameters(epochs=epoch,\n                                cache_prefix='',\n                                sample_interval=log_step,\n              \n                                labels=(0,1,2,3))\n  \n        num_classes=4\n      ### Calling the GAN model\n    cgan_model = CGAN\n      # CGAN training\n    cgan = cgan_model(gan_args,num_classes=num_classes)\n    cgan.train(data = df, train_arguments=train_args, num_cols=num_cols, cat_cols=cat_cols,label_col=label_col)\n\n\n    l1= []\n    for i in df[target_label].value_counts().items():\n        cond_array = np.array([i[0]])\n        l1.append(cgan.sample(n_samples=i[1],condition=cond_array))\n    df_cgan = shuffle(pd.concat(l1))\n    return df_cgan, mixed_df(df,df_cgan)\n\n","metadata":{"id":"fU3OiizT7lCO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_dragan_data(df,num_cols,cat_cols):\n  ### Calling the DRAGAN model\n  dragan_model = DRAGAN\n\n  #Training the draCGAN model\n  dragan = dragan_model(gan_args,n_discriminator=2)\n\n  dragan.train(data = df, train_arguments=train_args, num_cols=num_cols, cat_cols=cat_cols)\n  #Sampling the data\n\n  df_dragan = dragan.sample(5*len(df)).abs()\n  drop_some_data(df,df_dragan)\n  return df_dragan, mixed_df(df,df_dragan)\n","metadata":{"id":"6HQWvqFVK0Dr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_cramergan_data(df,num_cols,cat_cols):\n  ### Calling the CRAMERGAN model\n  cramergan_model = CRAMERGAN\n  # CRAMERGAN training\n\n  #Training the cramer model\n  cramergan = cramergan_model(gan_args)\n\n  cramergan.train(data = df, train_arguments=train_args, num_cols=num_cols, cat_cols=cat_cols)\n  df_cramergan = cramergan.sample(5*len(df)).abs()\n  drop_some_data(df,df_cramergan)\n  return df_cramergan, mixed_df(df,df_cramergan)\n\n\n","metadata":{"id":"p0oNxsf8MVt0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_ctgan_data(df,cat_cols,epoch):\n  ctgan = CTGANSynthesizer(epochs=epoch,\n        batch_size=100,\n        generator_dim=(256, 256, 256),\n        discriminator_dim=(256, 256, 256))\n  \n  ctgan.fit(df,cat_cols)\n  # We call model’s sample function to generate samples based on the learned model.\n  df_ctgan = ctgan.sample(4*len(df))\n  drop_some_data(df,df_ctgan)\n  return df_ctgan, mixed_df(df,df_ctgan)\n  ","metadata":{"id":"LBZ8SGzpV40T","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def highlighting_mean_greater(s):\n    return['background-color:#F7F7F7' if i%2==0 else 'background-color: ##FFDDDD' for (i,j) in enumerate(s)]\ndef save_results(Results_svm,Results_xgb,folder_name,n_splits,num_epochs):\n    R_svm = pd.DataFrame(Results_svm)\n    R_xgb = pd.DataFrame(Results_xgb)\n    \n    outFile = f'/kaggle/working/{folder_name}/Results_{folder_name.split()[0]}_CV={n_splits}.xlsx'\n    \n    if(os.path.exists(outFile)):\n        with pd.ExcelWriter(outFile ,mode=\"a\", engine=\"openpyxl\") as writer:\n            R_svm.T.style.apply(highlighting_mean_greater).to_excel(writer, engine='openpyxl', index=False, sheet_name=f'num-epochs={num_epochs}',startcol=7,startrow=6)\n        with pd.ExcelWriter(outFile ,mode=\"a\", engine=\"openpyxl\") as writer:\n            R_xgb.T.style.apply(highlighting_mean_greater).to_excel(writer, engine='openpyxl', index=False, sheet_name=f'num-epochs={num_epochs}xgb',startcol=17,startrow=6)\n\n    else:\n        with pd.ExcelWriter(outFile,engine=\"openpyxl\") as writer:\n            R_svm.T.style.apply(highlighting_mean_greater).to_excel(writer, engine='openpyxl', index=False, sheet_name=f'num-epochs={num_epochs}',startcol=7,startrow=6)\n            R_xgb.T.style.apply(highlighting_mean_greater).to_excel(writer, engine='openpyxl', index=False, sheet_name=f'num-epochs={num_epochs}',startcol=17,startrow=6)\n    \n    print(\"Data appended successfully.\")\n","metadata":{"id":"6F2CIDZ0efJc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results_svm= generate_results_dictionary(\"SVM\")\nResults_xgb = generate_results_dictionary(\"XGBoost\")\ntrain_evaulate_xgb (X_train, X_test, y_train, y_test,df_type='Original')\ntrain_evaulate_svm (X_train, X_test, y_train, y_test,df_type='Original')\nResults_svm_Original=Results_svm['Original']\nResults_xgb_Original=Results_xgb['Original']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n\nfor epoch in epochs:\n    Results_svm= generate_results_dictionary(\"SVM\")\n    Results_xgb = generate_results_dictionary(\"XGBoost\")\n    X_train, X_test, y_train, y_test = split_up(df)    \n\n    Results_svm['Original']=Results_svm_Original\n    Results_xgb['Original']=Results_xgb_Original\n    train_args = TrainParameters(epochs=epoch,\n                            sample_interval=log_step)\n    for Gmodel in  ['WGAN','GAN','CGAN','DRAGAN','CRAMERGAN','CTGAN']:\n        display(HTML('<h1Working With{} Epcochs In {}</h1>'.format(epoch,Gmodel)))    \n        if Gmodel=='WGAN':\n            working_df,mixed_working_df = generate_wgan_data(df,num_cols,cat_cols)\n        elif  Gmodel=='GAN':\n            working_df,mixed_working_df = generate_gan_data(df,num_cols,cat_cols)\n        elif  Gmodel=='CGAN':\n            if target_label =='Level':\n                working_df,mixed_working_df = generate_cgan_data(df,num_cols,['Gender'],target_label,epoch)\n            elif target_label =='NSP':\n                working_df,mixed_working_df = generate_cgan_data(df,num_cols,['Tendency', 'DS', 'DP'],target_label,epoch)\n            else :\n                working_df,mixed_working_df = generate_cgan_data(df,num_cols,[],target_label,epoch)\n        elif  Gmodel=='DRAGAN':\n            working_df,mixed_working_df = generate_cramergan_data(df,num_cols,cat_cols)\n        elif  Gmodel=='CRAMERGAN':\n            working_df,mixed_working_df = generate_dragan_data(df,num_cols,cat_cols)\n        elif  Gmodel=='CTGAN':\n            working_df,mixed_working_df = generate_ctgan_data(df,cat_cols,epoch)\n            \n#         plot_histogram(df,working_df)   \n#         plot_histogram(df,mixed_working_df)   \n#         print(working_df[target_label].value_counts())\n\n\n        working_df.to_csv(f'/kaggle/working/{folder_name}/df_{Gmodel}_{epoch}.csv')\n        mixed_working_df.to_csv(f'/kaggle/working/{folder_name}/mixed_{Gmodel}_{epoch}.csv') \n\n        X_train, X_test, y_train, y_test = split_up(working_df)\n        train_evaulate_xgb (X_train, X_test, y_train, y_test,df_type=Gmodel)\n        train_evaulate_svm (X_train, X_test, y_train, y_test,df_type=Gmodel)   \n\n        X_train, X_test, y_train, y_test = split_up(mixed_working_df)\n        train_evaulate_xgb (X_train, X_test, y_train, y_test,df_type=f'{Gmodel}_Mixed')\n        train_evaulate_svm (X_train, X_test, y_train, y_test,df_type=f'{Gmodel}_Mixed') \n        \n    save_results(Results_svm,Results_xgb,folder_name,n_splits,epoch)\n#     save_results(Results_xgb,'xgb',folder_name,n_splits,epoch)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results_svm","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}